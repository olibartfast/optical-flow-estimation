{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'matplotlib'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m get_ipython()\u001b[39m.\u001b[39;49mrun_line_magic(\u001b[39m'\u001b[39;49m\u001b[39mmatplotlib\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39minline\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
            "File \u001b[0;32m~/repos/torchscript_RAFT_optical_flow/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2417\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2415\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mlocal_ns\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2416\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2417\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2419\u001b[0m \u001b[39m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2420\u001b[0m \u001b[39m# when using magics with decodator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2421\u001b[0m \u001b[39m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2422\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(fn, magic\u001b[39m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[39mFalse\u001b[39;00m):\n",
            "File \u001b[0;32m~/repos/torchscript_RAFT_optical_flow/.venv/lib/python3.10/site-packages/IPython/core/magics/pylab.py:99\u001b[0m, in \u001b[0;36mPylabMagics.matplotlib\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAvailable matplotlib backends: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m backends_list)\n\u001b[1;32m     98\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 99\u001b[0m     gui, backend \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshell\u001b[39m.\u001b[39;49menable_matplotlib(args\u001b[39m.\u001b[39;49mgui\u001b[39m.\u001b[39;49mlower() \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(args\u001b[39m.\u001b[39;49mgui, \u001b[39mstr\u001b[39;49m) \u001b[39melse\u001b[39;49;00m args\u001b[39m.\u001b[39;49mgui)\n\u001b[1;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_show_matplotlib_backend(args\u001b[39m.\u001b[39mgui, backend)\n",
            "File \u001b[0;32m~/repos/torchscript_RAFT_optical_flow/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3588\u001b[0m, in \u001b[0;36mInteractiveShell.enable_matplotlib\u001b[0;34m(self, gui)\u001b[0m\n\u001b[1;32m   3567\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39menable_matplotlib\u001b[39m(\u001b[39mself\u001b[39m, gui\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   3568\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Enable interactive matplotlib and inline figure support.\u001b[39;00m\n\u001b[1;32m   3569\u001b[0m \n\u001b[1;32m   3570\u001b[0m \u001b[39m    This takes the following steps:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3586\u001b[0m \u001b[39m        display figures inline.\u001b[39;00m\n\u001b[1;32m   3587\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3588\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib_inline\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend_inline\u001b[39;00m \u001b[39mimport\u001b[39;00m configure_inline_support\n\u001b[1;32m   3590\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m pylabtools \u001b[39mas\u001b[39;00m pt\n\u001b[1;32m   3591\u001b[0m     gui, backend \u001b[39m=\u001b[39m pt\u001b[39m.\u001b[39mfind_gui_and_backend(gui, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpylab_gui_select)\n",
            "File \u001b[0;32m~/repos/torchscript_RAFT_optical_flow/.venv/lib/python3.10/site-packages/matplotlib_inline/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m backend_inline, config  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m      2\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m0.1.6\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# noqa\u001b[39;00m\n",
            "File \u001b[0;32m~/repos/torchscript_RAFT_optical_flow/.venv/lib/python3.10/site-packages/matplotlib_inline/backend_inline.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"A matplotlib backend for publishing figures via display_data\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Copyright (c) IPython Development Team.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# Distributed under the terms of the BSD 3-Clause License.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m colors\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackends\u001b[39;00m \u001b[39mimport\u001b[39;00m backend_agg\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
          ]
        }
      ],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Optical Flow: Predicting movement with the RAFT model\n",
        "\n",
        "Optical flow is the task of predicting movement between two images, usually two\n",
        "consecutive frames of a video. Optical flow models take two images as input, and\n",
        "predict a flow: the flow indicates the displacement of every single pixel in the\n",
        "first image, and maps it to its corresponding pixel in the second image. Flows\n",
        "are (2, H, W)-dimensional tensors, where the first axis corresponds to the\n",
        "predicted horizontal and vertical displacements.\n",
        "\n",
        "The following example illustrates how torchvision can be used to predict flows\n",
        "using our implementation of the RAFT model. We will also see how to convert the\n",
        "predicted flows to RGB images for visualization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "\n",
        "plt.rcParams[\"savefig.bbox\"] = \"tight\"\n",
        "\n",
        "\n",
        "def plot(imgs, **imshow_kwargs):\n",
        "    if not isinstance(imgs[0], list):\n",
        "        # Make a 2d grid even if there's just 1 row\n",
        "        imgs = [imgs]\n",
        "\n",
        "    num_rows = len(imgs)\n",
        "    num_cols = len(imgs[0])\n",
        "    _, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)\n",
        "    for row_idx, row in enumerate(imgs):\n",
        "        for col_idx, img in enumerate(row):\n",
        "            ax = axs[row_idx, col_idx]\n",
        "            img = F.to_pil_image(img.to(\"cpu\"))\n",
        "            ax.imshow(np.asarray(img), **imshow_kwargs)\n",
        "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reading Videos Using Torchvision\n",
        "We will first read a video using :func:`~torchvision.io.read_video`.\n",
        "Alternatively one can use the new :class:`~torchvision.io.VideoReader` API (if\n",
        "torchvision is built from source).\n",
        "The video we will use here is free of use from [pexels.com](https://www.pexels.com/video/a-man-playing-a-game-of-basketball-5192157/),\n",
        "credits go to [Pavel Danilyuk](https://www.pexels.com/@pavel-danilyuk).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "from pathlib import Path\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "\n",
        "video_url = \"https://download.pytorch.org/tutorial/pexelscom_pavel_danilyuk_basketball_hd.mp4\"\n",
        "video_path = Path(tempfile.mkdtemp()) / \"basketball.mp4\"\n",
        "_ = urlretrieve(video_url, video_path)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":func:`~torchvision.io.read_video` returns the video frames, audio frames and\n",
        "the metadata associated with the video. In our case, we only need the video\n",
        "frames.\n",
        "\n",
        "Here we will just make 2 predictions between 2 pre-selected pairs of frames,\n",
        "namely frames (100, 101) and (150, 151). Each of these pairs corresponds to a\n",
        "single model input.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchvision.io import read_video\n",
        "frames, _, _ = read_video(str(video_path), output_format=\"TCHW\")\n",
        "\n",
        "img1_batch = torch.stack([frames[100], frames[150]])\n",
        "img2_batch = torch.stack([frames[101], frames[151]])\n",
        "\n",
        "plot(img1_batch)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The RAFT model accepts RGB images. We first get the frames from\n",
        ":func:`~torchvision.io.read_video` and resize them to ensure their dimensions\n",
        "are divisible by 8. Note that we explicitly use ``antialias=False``, because\n",
        "this is how those models were trained. Then we use the transforms bundled into\n",
        "the weights in order to preprocess the input and rescale its values to the\n",
        "required ``[-1, 1]`` interval.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchvision.models.optical_flow import Raft_Large_Weights\n",
        "\n",
        "weights = Raft_Large_Weights.DEFAULT\n",
        "transforms = weights.transforms()\n",
        "\n",
        "\n",
        "def preprocess(img1_batch, img2_batch):\n",
        "    img1_batch = F.resize(img1_batch, size=[520, 960], antialias=False)\n",
        "    img2_batch = F.resize(img2_batch, size=[520, 960], antialias=False)\n",
        "    return transforms(img1_batch, img2_batch)\n",
        "\n",
        "\n",
        "img1_batch, img2_batch = preprocess(img1_batch, img2_batch)\n",
        "\n",
        "print(f\"shape = {img1_batch.shape}, dtype = {img1_batch.dtype}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Estimating Optical flow using RAFT\n",
        "We will use our RAFT implementation from\n",
        ":func:`~torchvision.models.optical_flow.raft_large`, which follows the same\n",
        "architecture as the one described in the [original paper](https://arxiv.org/abs/2003.12039).\n",
        "We also provide the :func:`~torchvision.models.optical_flow.raft_small` model\n",
        "builder, which is smaller and faster to run, sacrificing a bit of accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchvision.models.optical_flow import raft_large\n",
        "\n",
        "# If you can, run this example on a GPU, it will be a lot faster.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = raft_large(weights=Raft_Large_Weights.DEFAULT, progress=False).to(device)\n",
        "model = model.eval()\n",
        "\n",
        "list_of_flows = model(img1_batch.to(device), img2_batch.to(device))\n",
        "print(f\"type = {type(list_of_flows)}\")\n",
        "print(f\"length = {len(list_of_flows)} = number of iterations of the model\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The RAFT model outputs lists of predicted flows where each entry is a\n",
        "(N, 2, H, W) batch of predicted flows that corresponds to a given \"iteration\"\n",
        "in the model. For more details on the iterative nature of the model, please\n",
        "refer to the [original paper](https://arxiv.org/abs/2003.12039). Here, we\n",
        "are only interested in the final predicted flows (they are the most acccurate\n",
        "ones), so we will just retrieve the last item in the list.\n",
        "\n",
        "As described above, a flow is a tensor with dimensions (2, H, W) (or (N, 2, H,\n",
        "W) for batches of flows) where each entry corresponds to the horizontal and\n",
        "vertical displacement of each pixel from the first image to the second image.\n",
        "Note that the predicted flows are in \"pixel\" unit, they are not normalized\n",
        "w.r.t. the dimensions of the images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "predicted_flows = list_of_flows[-1]\n",
        "print(f\"dtype = {predicted_flows.dtype}\")\n",
        "print(f\"shape = {predicted_flows.shape} = (N, 2, H, W)\")\n",
        "print(f\"min = {predicted_flows.min()}, max = {predicted_flows.max()}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing predicted flows\n",
        "Torchvision provides the :func:`~torchvision.utils.flow_to_image` utlity to\n",
        "convert a flow into an RGB image. It also supports batches of flows.\n",
        "each \"direction\" in the flow will be mapped to a given RGB color. In the\n",
        "images below, pixels with similar colors are assumed by the model to be moving\n",
        "in similar directions. The model is properly able to predict the movement of\n",
        "the ball and the player. Note in particular the different predicted direction\n",
        "of the ball in the first image (going to the left) and in the second image\n",
        "(going up).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchvision.utils import flow_to_image\n",
        "\n",
        "flow_imgs = flow_to_image(predicted_flows)\n",
        "\n",
        "# The images have been mapped into [-1, 1] but for plotting we want them in [0, 1]\n",
        "img1_batch = [(img1 + 1) / 2 for img1 in img1_batch]\n",
        "\n",
        "grid = [[img1, flow_img] for (img1, flow_img) in zip(img1_batch, flow_imgs)]\n",
        "plot(grid)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bonus: Creating GIFs of predicted flows\n",
        "In the example above we have only shown the predicted flows of 2 pairs of\n",
        "frames. A fun way to apply the Optical Flow models is to run the model on an\n",
        "entire video, and create a new video from all the predicted flows. Below is a\n",
        "snippet that can get you started with this. We comment out the code, because\n",
        "this example is being rendered on a machine without a GPU, and it would take\n",
        "too long to run it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# from torchvision.io import write_jpeg\n",
        "# for i, (img1, img2) in enumerate(zip(frames, frames[1:])):\n",
        "#     # Note: it would be faster to predict batches of flows instead of individual flows\n",
        "#     img1, img2 = preprocess(img1, img2)\n",
        "\n",
        "#     list_of_flows = model(img1.to(device), img2.to(device))\n",
        "#     predicted_flow = list_of_flows[-1][0]\n",
        "#     flow_img = flow_to_image(predicted_flow).to(\"cpu\")\n",
        "#     output_folder = \"/tmp/\"  # Update this to the folder of your choice\n",
        "#     write_jpeg(flow_img, output_folder + f\"predicted_flow_{i}.jpg\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once the .jpg flow images are saved, you can convert them into a video or a\n",
        "GIF using ffmpeg with e.g.:\n",
        "\n",
        "ffmpeg -f image2 -framerate 30 -i predicted_flow_%d.jpg -loop -1 flow.gif\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
